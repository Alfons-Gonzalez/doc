
Vamos a instalar un cluster de elasticsearch compuesto por tres máquinas:
quercus (Dell R310), kabul (Dell R210) i sefarad (Dell R210)

Como de las tres quercus es quien tiene más memória y con diferencia, será esta
donde instalaremos las web que nos permitirán interactuar con elasticsearch:
kibana, grafana y elasticsearch-HQ (permite ver el estado del cluster).

 Indice
========

1.- Elasticsearch
	1.1.- Java
	1.2.- Elementos básicos
	1.3.- Puertos
	1.4.- Ficheros de configuración
	1.5.- X-Pack
	1.6.- Instalación
	1.7.- Configuracion
		1.7.1.- Activar el modo Cluster
	1.8.- Fichero de log
	1.9.- Interactuar con la consola
	1.10.- Firewall final

2.- Elasticsearch-HQ
	2.1.- Instalación
	2.2.- Puerto
	2.3.- Modo produccion
	2.4.- Fichero de log

3.- Kibana
	3.1.- Instalacion
	3.2.- Puertos
	3.3.- Configuracion
	3.4.- Fichero de log

4.- Grafana
	4.1.- Instalacion
	4.2.- Puertos
	4.3.- Configuracion
	4.4.- Fichero de log


5.- Apache ProxyPass 

6.- Beats: FileBeat y MetricBeat


1.- Elasticsearch
=================

Se intala con RPM mediante el repo oficial. Usando este repo podemos instalar mediante
yum el elasticsearch, kibana, logstash, los beats, etc. 

1.1.- Java
-----------

IMPORTANTE: Usa java y se recomienda hacer servir la versión de java de oracle y para
evitar problemas, que en todos los nodos del cluster se tenga la misma versión. Yo puse
el último estable: 

[root@quercus html]# java -version
java version "1.8.0_191"
Java(TM) SE Runtime Environment (build 1.8.0_191-b12)
Java HotSpot(TM) 64-Bit Server VM (build 25.191-b12, mixed mode)
[root@quercus html]# 

1.2.- Elementos básicos
-----------------------

Es importante tener claro lo que son los Documents, indices, shards

    Documento: el tipo de objeto que guardamos.

    Indice: cada Documento tiene su indice que no tiene por que caber en una solo 
	nodo: puede estar distribuido y resplicado en varios nodos para tener alta 
	disponibilidad, mejor rendimiento (acceso en paralelo), etc.
    Shards: un índice se divide en X shards (siendo X => 1). Los shards pueden estar 
	distribuidos en los n nodos lo que permite acceso paralelo.
    Replica: cuantas copias de un índice tenemos en el cluster. Permite alta disponibilidad 
	en caso de petada de un nodo y ayuda al acceso en paralelo.
    By default: cuando un nuevo Index es creado, si no se especifica lo contrario, este 
	es creado con shards=5 y replica=1.

Como con Slurm, en todos los nodos estos ficheros han de estar iguales !!

1.3.- Puertos
-------------

    9200 -> Puerto http. Es este el puerto que usarán aplicaciones como kibana, grafana,
	para comunicarse con el elasticsearch.
    9300 -> Puerto de transporte. Este puerto es importante ya que es el que hacen 
	los nodos del elasticsearch para comunicarse entre ellos (yo no lo tenía abierto 
	y no se podrían ver por lo que no estaban en modo cluster !!)

1.4.- Ficheros de configuración
-------------------------------

Directorio principal de configuración: /etc/elasticsearch

    * elasticsearch.yml : fichero general de configuración. Algunas cosas como el 
	logging se pueden poner o bien aquí o en el fichero de configuración de java.
    * jvm.options: configuración de los parámetros de java. Tal vez el más importante sea
	el tamaño del heape space.
    * log4j2.properties: logging configuration parameters.

1.5.- X-Pack
------------    

El módulo del X-Pack se autoinstala con una configuración básica pero es de pago si queremos
hacer servir opciones avanzadas.

Por lo que vi, entre otras coas, es el que permite configurar opciones de seguridad (que las 
conexiones entre los nodos y webs de monitorización/gestion vaya por ssl), que kibana pida para 
entrar usuario y passords, la creación de usuarios, machine learning, configurar alarmas en 
el Kibana, etc. Por defecto viene activado con una configuración básica que és la gratuita.
OJO, que si lo activamos por equivocacion, es con licencia de 30 dias y luego nos tendremos 
que acordar de desactivar. La info dice de este módulo:

"X-Pack is an Elastic Stack extension that provides security, alerting, monitoring, reporting, 
machine learning, and many other capabilities. By default, when you install Elasticsearch, 
X-Pack is installed." y  yo añado "... si, installed, pero es de pago (los manguis no lo dejan 
claro en la docu).

1.6.- Instalación
-----------------

Para hacer la instalación, lo hice así: 

a.- Si se instaló con el sistema algún interprete de java
del repo, lo quitamos para que solo esté el de Oracle (evitamos
posibles confuciones/conflictos).

b.- Instalamos el último jdk estable de Oracle, yo instalé este:

[root@quercus html]# java -version
java version "1.8.0_191"
Java(TM) SE Runtime Environment (build 1.8.0_191-b12)
Java HotSpot(TM) 64-Bit Server VM (build 25.191-b12, mixed mode)
[root@quercus html]# 

c.- Añadimos el repo de elasticsearch:

[root@quercus ~]# cat /etc/yum.repos.d/elasticsearch.repo 
[elasticsearch-6.x]
name=Elasticsearch repository for 6.x packages
baseurl=https://artifacts.elastic.co/packages/6.x/yum
gpgcheck=1
gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch
enabled=1
autorefresh=1
type=rpm-md
[root@quercus ~]# 

d.- Instalamos el elasticsearch: 

yum install elasticsearch -y

1.7.- Configuracion
-------------------

Y aquí es donde está la chicha del asunto. Hemos de hacer cambio en
el fichero de configuración del elasticsearch y a nivel de sistema 
operativo. La gran ventaja de hacer la instalación mediante yum y no
mediante los .tar.gz, es que el rpm ya nos deja el sistema casi 
preparado (es el fichero del systemd que gestiona el servicio quien
se encarga de hacer los cambios necesarios en el sistema):

a.- (Opcinoal) Si queremos ver la info del elasticsearch mediante el
comando del sistema journalctld:

- Editamos el fichero del servicio del systemd (/usr/lib/systemd/system/elasticsearch.service)
y quitamos '--quiet' a la linea del ExecStart:

#ExecStart=/usr/share/elasticsearch/bin/elasticsearch -p ${PID_DIR}/elasticsearch.pid --quiet
ExecStart=/usr/share/elasticsearch/bin/elasticsearch -p ${PID_DIR}/elasticsearch.pid

b.- Aumentamos el heap de java de 1Gb a 4GB. Siempre se ha de poner el min i el max 
con el mismo tamaño. No pasar del 50% de la memoria física (hay una web de la docu 
con más recomendaciones sobre estos tamaños). Esto se pone en el fichero

[root@quercus ~]# cat /etc/elasticsearch/jvm.options 
## JVM configuration

################################################################
## IMPORTANT: JVM heap size
################################################################
##
## You should always set the min and max JVM heap
## size to the same value. For example, to set
## the heap to 4 GB, set:
##
## -Xms4g
## -Xmx4g
##
## See https://www.elastic.co/guide/en/elasticsearch/reference/current/heap-size.html
## for more information
##
################################################################

# Xms represents the initial size of total heap space
# Xmx represents the maximum size of total heap space

-Xms4g
-Xmx4g

...
...
...

- En los tres nodos del cluster del GRIB puse 4GB (en ninguno de ellos era más del 50%
de la memória física) pero entiendo que este valor puede ser personalizado para
cada ordenador ya que pueden tener configuraciones de memoria muy diferentes. En el
cluster del SIT que es una máquina virtual pequeñita puse 1GB.

c.- Como se instaló usando RPM y yum, todos las cambios a nivel de sistema que pide que 
se hagan en la docu de elasticsearch ya quedaban hechos. Diria que todos estos cambios
están definidos directamente en la parte final del fichero de configuración del 
servicio del systemd:

/usr/lib/systemd/system/elasticsearch.service

- La única configuración que tube que hacer y que no estaba dentro de este fichero 
fué desactivar el uso de swap por parte del elasticsearch. Se ve que es importante 
para evitar que tenga que ir al swap a buscar cosas. La docu explica varias maneras de hacer
esto, al final apliqué la que llaman "Enable bootstrap.memory_lock" que consisitió en
estos dos pasos:

c.1.- Descomentar y poner a true la siguiente directiva en el fichero de configuración del
elasticsearch (/etc/elasticsearch/elasticsearch.yml):

bootstrap.memory_lock: true

c.2.-  Crear el fichero: /etc/systemd/system/elasticsearch.service.d/override.conf con
el siguiente contenido:

[Service]
LimitMEMLOCK=infinity

y decirle al systemd que actualice los cambios:

systemctl daemon-reload

(también se puede hacer ejecutando: systemctl edit elasticsearch.service)

y hacer un restart del servicio. Para ver que s'ha desactivat, en el kibana podem executar
aquesta comanda y es a de retornar un 'true' (aixó d'interactuar amb el elasticsearch des de
la consola del kibana s'explica més endavant):

GET _nodes?filter_path=**.mlockall

d.- Que el nombre del host sea el HOSTNAME de la máquina:

[root@quercus ~]# grep node.name /etc/elasticsearch/elasticsearch.yml 
node.name: ${HOSTNAME}
[root@quercus ~]# 

e.- Para dejar de estar en modo deployment y entrar a funcionar en modo
production, hay que configurar el network.host con la ip del nodo (by default, 
usa la de loopback). Al hacer esto, salimos del modo deployment y entramos 
en el modo production. Este detalles es muy importante ya que esto implica 
que ciertos errores de config, que en modo deployment son warnings, pasan a 
ser errores críticos y el servicio no arrancará:

[root@quercus ~]# grep network.host /etc/elasticsearch/elasticsearch.yml
network.host: 172.20.16.10
[root@quercus ~]# 

Si queremos trabajar con la dirección de loopback pero entrando en modo 
production hemos de activar una directiva en el fichero de configuración:

	es.enforce.bootstrap.checks: true

Es posible tener varias instáncias de elasticsearch en la propia máquina, 
todas ellas usando la dirección de loopback (pero diferente puerto) y que 
trabajen en modo cluster. Esto se usa a modo de pruebas o desarrollo, no
tiene sentido montar así un cluster de produccion por el mal rendimiento
que tendríamos.

1.7.1.- Activar el modo Cluster
-------------------------------

SIT: Como en el SIT solo hay un nodo en el cluster no fué necesario
aplicar toda esta configuración.

Para que los tres nodos del GRIB empiecen a trabajar en modo cluster:

a.- IMPORTANTE: cambiamos el cluster name !! El hecho de que varios nodos empiecen a
funcionar en modo cluster, básicamente es por que:

- Todos ellos tienen el mismo clustername.
- Pueden hablar entre ellos mediante el puerto 9300.

[root@quercus ~]# grep cluster.name /etc/elasticsearch/elasticsearch.yml 
cluster.name: grib-elastic
[root@quercus ~]# 

b.- En la docu recomiendan que es mejor hacer un unicast en vez de un broadcast para
que los nodos del cluster se encuentren entre ellos. Para ello, hemos de activar y poner
lo siguiente en el fichero de configuración del elasticsearch:

[root@quercus ~]# grep discovery.zen.ping.unicast.hosts /etc/elasticsearch/elasticsearch.yml 
discovery.zen.ping.unicast.hosts: ["172.11.61.27", "172.31.22.131","172.31.32.221"]
[root@quercus ~]# 

Entre ellos deciden quien es el master (by default todos lo pueden ser). Se puede
forzar a que cada nodo tenga el role que nos interese indicándolo en el fichero de configuración:

https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html

c.- Otro detalle importante es configurar una directiva para evitar un “Split Brain”. 
Esto es que si hay un error de red y algún nodo del cluster queda incomunicado del resto, 
este puede decidir que los otros han caido, coja el role de master y siga insertando info 
y que cuando el problema de red desaparezca y vayan a sincronizar sus datos se lie parda para
decidir quien de los masters (el que se quedó incomunicado y el que tenia el role de master
) tiene los datos correctos ya que los dos masters querrán tener la razón.

Para que esto no pase se le indica un número mínimo de nodos en el cluster: si no se cumple, 
se para todo para evitar que se produzca un "Split Brain":

discovery.zen.minimum_master_nodes: 2

La regla es: N/2 + 1. N (round down the result to the nearest integer)

d.- Reiniciar los nodos. En el siguiente apartado se explica como interactuar con la consola
y así comprovar que estan trabajando en modo cluster.

1.8.- Fichero de log
--------------------

Los ficheros de log están en el directorio:

/var/log/elasticsearch

[root@quercus elasticsearch-HQ]# ls /var/log/elasticsearch/
gc.log.0.current         grib-elastic_audit.log        grib-elastic_index_indexing_slowlog.log  grib-elastic.log
grib-elastic_access.log  grib-elastic_deprecation.log  grib-elastic_index_search_slowlog.log
[root@quercus elasticsearch-HQ]# 

1.9.- Interactuar con la consola
--------------------------------

Las querys son en formato json. Tenemos dos maneras para interactuar con 
el elasticsearch:

a.- Por lineas de comandos usando la herramienta 'curl':

curl -X <action> "<elastic_server>:<port>/<request>"

ej:

curl -X GET "quercus.prib.upf.edu:9200/_cat/nodes?v"

b.- Usando la herramienta "Dev Tools" del Kibana (está en el menú de la izquierda)
y metemos direactemte la consulta:

GET /_cat/nodes?v

Algunes consultas útiles que use son:

- Estat del cluster: 

GET _cluster/health

- El mateix pero en otro formato (creo que da un poco más de info):

GET /_cat/health?v

- Ver la lista de nodos que forman el cluster y su estado: 

GET /_cat/nodes?v

- Obtener la configuración completa del cluster:

GET /_cluster/settings?include_defaults=true

- Obtener información de todos los índices con su estado:

GET /_cat/indices?v

* Configuración de todos los indices:

GET /_all/_settings

* Configuracion de solo un indice (o algunos con wildcards):

GET /metricbeat-*/_settings

* Con algún filtro: ver solo el valor index->number*

GET /metricbeat-*/_settings/index.number_*

También se puede hacer servir PUT que implica hacer cambios
en algún indice o configuración.

1.10.- Firewall final
--------------------

La configuración final del firewall que quedo en el nodo quercus del 
cluster del GRIB fue esta (este nodo es que que lleva el servicio de kibana,
grafana i elasticsearch-HQ):

[root@quercus ~]# firewall-cmd --list-all
public (active)
  target: default
  icmp-block-inversion: no
  interfaces: em1
  sources: 
  services: 
  ports: 
  protocols: 
  masquerade: no
  forward-ports: 
  source-ports: 
  icmp-blocks: 
  rich rules: 
	rule family="ipv4" source address="172.22.2.234" service name="ssh" accept
	rule family="ipv4" source address="172.20.16.0/24" service name="ssh" accept
	rule family="ipv4" source address="172.20.16.0/24" service name="kibana" accept
	rule family="ipv4" source address="172.20.16.0/24" service name="grafana" accept
	rule family="ipv4" source address="172.20.16.0/24" service name="elasticsearch" accept
	rule family="ipv4" source address="172.20.16.25" service name="bacula-client" accept
	rule family="ipv4" source address="172.20.16.0/24" service name="http" accept
	rule family="ipv4" source address="172.20.16.222" service name="dell-omsa" accept
	rule family="ipv4" source address="172.20.16.221" service name="dell-omsa" accept
	rule family="ipv4" source address="172.20.16.0/24" service name="elastichq" accept
	rule family="ipv4" source address="84.89.134.128/25" service name="elasticsearch" accept
	rule family="ipv4" source address="84.89.134.128/25" service name="kibana" accept
[root@quercus ~]# 

y en los otros dos nodos del cluster (no ofrecen el servicio de kibana, grafana i elasticsearch-HQ):

[root@kabul ~]# firewall-cmd --list-all
public (active)
  target: default
  icmp-block-inversion: no
  interfaces: em1
  sources: 
  services: 
  ports: 
  protocols: 
  masquerade: no
  forward-ports: 
  source-ports: 
  icmp-blocks: 
  rich rules: 
	rule family="ipv4" source address="172.20.16.0/24" service name="ssh" accept
	rule family="ipv4" source address="172.20.16.0/24" service name="elasticsearch" accept
	rule family="ipv4" source address="172.22.16.25" service name="bacula-client" accept
	rule family="ipv4" source address="172.20.16.222" service name="dell-omsa" accept
	rule family="ipv4" source address="172.20.16.221" service name="dell-omsa" accept
	rule family="ipv4" source address="84.89.134.128/25" service name="elasticsearch" accept
[root@kabul ~]# 

y el iptables de la máquina con el elasticsearch del SIT (es mas sencillo por que no tiene que 
comunicarse con otros nodos del cluster de elasticsearch):

[root@sit-metrics ~]# firewall-cmd --list-all
public (active)
  target: default
  icmp-block-inversion: no
  interfaces: eth0
  sources: 
  services: ssh dhcpv6-client elasticsearch kibana grafana http
  ports: 
  protocols: 
  masquerade: no
  forward-ports: 
  source-ports: 
  icmp-blocks: 
  rich rules: 
	
[root@sit-metrics ~]# 

Como detalle, para generar las reglas se hacen servir las definiciones de los servivios
que usar el firewalld. Para ver los puertos que implican estos servicios hemos de ver su
fichero de configuracion que están aquí:

/usr/lib/firewalld/services/

[root@kabul ~]# cat /usr/lib/firewalld/services/kibana.xml 
<?xml version="1.0" encoding="utf-8"?>
<service>
  <short>Kibana</short>
  <description>Kibana is an open source data visualization platform that allows you to interact with your data through stunning, powerful graphics that can be combined into custom dashboards that help you share insights from your data far and wide.</description>
  <port protocol="tcp" port="5601"/>
</service>
[root@kabul ~]# 


(algunos los cree yo por que me gusta más que se vea así, queda mas aclaratorio a que
puerto hace referencia la regla).

2.- Elasticsearch-HQ
====================

SIT: Esta herramienta no esta instalada en el SIT ya que el elasticsearch
esta solo formado por un nodo (no era necesario).

Es una herramienta web que nos permite de un modo sencillo ver el estado del
cluster, de los indices y de donde están los shards y sus réplicas. Esta hecho
en python y no corre sobre apache (esto no me gusta pero es así como funciona).

Según la docu no solo permite monitorizar el estado del cluster, también nos 
permite modificar, añadir o eliminar indices pero no lo he probado.

2.1.- Instalación
-----------------

Las instrucciones de como se hace la instalación aquí:

http://docs.elastichq.org/installation.html#quick-start-guide

Antes de seguir los pasos de instalación, instalé el python-3.4 en la máquina
(la docu dice que xuta con python >= 3.4) y seguí estos pasos:

0.- Install pyton3.4: yum install python34 python34-pip -y
1.- Download or clone the repository https://github.com/ElasticHQ/elasticsearch-HQ
	to the directory /usr/local.
2.- Navigate to the root of the repository at /usr/local/elasticsearch-HQ and: 
	a.- comment out the line in the requirements.txt: gunicorn==19.7.1
	b.- pip3.4 install -r requirements.txt
3.- Start the server: python3.4 application.py
4.- Point your browser to: http://localhost:5000

2.2.- Puerto
------------

Escucha por el puerto 5000 y por el localhost (no miro como cambiarlo, ya haré
Luego un proxypass).

2.3.- Modo produccion
---------------------

Para ponerlo en produccion, no nos sirve el paso 3 anterior, hemos de ejecutar:

[root@quercus ~]# cat start_elasticsearch-HQ.sh 
/bin/python3 /bin/gunicorn --daemon --chdir /usr/local/elasticsearch-HQ -w 1 -b :5000 --worker-class eventlet application:application
[root@quercus ~]#

Y hacemos que se ejecute en el arranque:

root@quercus elasticsearch-HQ]# chmod u+x /etc/rc.d/rc.local
[root@quercus elasticsearch-HQ]# systemctl start rc-local
[root@quercus elasticsearch-HQ]# systemctl enable rc-local
[root@quercus elasticsearch-HQ]# echo "/root/start_elasticsearch-HQ.sh" >> /etc/rc.d/rc.local 

2.4.- Fichero de log
--------------------

El fitxer de log esta aqui:

/usr/local/elasticsearch-HQ/application.log

Creo que las opciones de log están en este fichero:

/usr/local/elasticsearch-HQ/elastichq/config/logger.json

3.- Kibana
===========

Solo lo instalamos en una máquina del cluster. Esta máquina es quercus por que es la que tiene más
memoria (asumo que es la que puede llegar a soportar más carga). En el SIT, como solo hay una máquina
se instala en esa.

Como no tenemos activado (por que es de pago) el módulo de X-Pack, no podemos configurar usuarios
para que el acceso quede más restringido (pondremos un user + password de apache).

A pesar de las limitaciones por no instalar el X-Pack, es interesante esta herramienta ya que nos
ofrece varias opciones interesantes (frame de la izquierda):

- DevTools: una consola con la que interactuar con el elasticsearch (en este documento hay una mini-lista
de comandos). Son instrucciones en formato json.

- Discover: ver como van las inserciones de info en el elasticsearch por parte de los clientes de
beats (metricbeat y filebeat) o logstash.

A parte de que los beats (filebeat y metricbeat) autoinstalan unos dashboards con los que ya podemos empezar
a ver algo.

3.1.- Instalacion
-----------------

Se instala con yum (se supone que con aterioridad ya hemos configurado el repo para yum de elasticsearch)

yum install kibana -y

3.2.- Puertos
-------------

Por defecto usa el puerto 5601 (mas adelante ya pondremos un proxypass). 

http://localhost:5601

3.3.- Configuracion
-------------------

Su fichero de configuración es:

/etc/kibana/kibana.yml

Y hacemos estos cambios

1.- Dejamos el puerto by default:

#server.port: 5601

2.- Para que sea accesible por otras máquinas, ponemos el nombre del 
host (si no, solo xuta por la dirección de loopback):

server.host: "quercus.prib.upf.edu"

3.- Donde está el elasticsearch con el que se tiene que comunicar:

elasticsearch.url: "http://quercus.prib.upf.edu:9200"

4.- Le pedimos que genere un fichero de log (por defecto no lo hace):

logging.dest: /var/log/kibana.log

Por último, al poner el proxypass para que esta web contestase a la url:

http://quercus.prib.upf.edu/kibana

se le tubo a hacer una modificacion mas con esta directiva:

server.basePath: "/kibana"

pero la pega fué que dejó de reponder correctamente mediante el puerto 5601.


3.4.- Logs
----------

Ficheros de log:

/var/log/kibana.log

4.- Grafana
===========

La ventaja que nos ofrece Grafana respecto a Kibana es que permite definir usuarios y permite también definir
alertas (para hacer esto con kibana hemos de tener el X-Pack habilitado pero este módulo es de pago).

Igual que el Kibana, lo instalamos solo en quercus que tiene bastante más memoria que el resto de los
nodos.

4.1.- Instalacion
-----------------

Igual que con las herramientas del elasticsearch, lo más fácil es instalar el repo
de grafana para yum es instalarlo con yum:

[root@quercus ~]# cat /etc/yum.repos.d/grafana.repo 
[grafana]
name=grafana
baseurl=https://packagecloud.io/grafana/stable/el/7/$basearch
repo_gpgcheck=1
enabled=1
gpgcheck=1
gpgkey=https://packagecloud.io/gpg.key https://grafanarel.s3.amazonaws.com/RPM-GPG-KEY-grafana
sslverify=1
sslcacert=/etc/pki/tls/certs/ca-bundle.crt
[root@quercus ~]# 

e instalamos con yum:

[root@quercus ~]# yum install grafana

y para activar el servicio:

[root@quercus grafana]# systemctl enable grafana-server

[root@quercus grafana]# systemctl start grafana-server


4.2.- Puertos
-------------

El puerto que hace servir por defecto es el localhost:3000

No me molesto en cambiar ni el puerto ni el hostname (que en vez de usar
la direccion de loopback, que use la ip de la máquina) por que pondremos un
proxypass.

4.3.- Configuracion
--------------------

No es necesario indicarle a través del fichero de configuración donde está la 
fuente de datos del elasticsearch, esto se hace desde la propia interficie web.

El fichero de configuración es:

/etc/grafana/grafana.ini

El únic que es va canviar va ser aixó per que contesti per aquesta nom: 

# The public facing domain name used to access grafana from a browser
domain = quercus.prib.upf.edu

Y una vez activado el proxypass en el apache de quercus, se tubo que cambiar
esto en el fichero de configuración para que funcionase:

# The full public facing url you use in browser, used for redirects and emails
# If you use reverse proxy and sub path specify full url (with sub path)
root_url = http://quercus.prib.upf.edu/grafana

4.4.- Logs
----------

Els fitxers de logs están aquí:

[root@quercus ~]# ll /var/log/grafana/grafana.log 
-rw-r--r-- 1 grafana grafana 4390 Jan 14 12:50 /var/log/grafana/grafana.log
[root@quercus ~]# 

5.- Apache ProxyPass
====================

Para no tener que conectar directamente con los puertos de kibana y grafana
se configuraron unos proxypass para que todo fuera por el puerto 80 (aunque
los puertos que usan estos servicios contiuan estando abiertos). También se 
aprovecha para decirle al apache que pida contraseña para entrar al Kibana y
al Elasticsearch-HQ. 

El problema es que para que funcionen bien estos proxypass, se han de hacer 
unos cambios en los ficheros de configuración del kibana y el grafana que provocan 
que dejen de funcionar bien si nos intentamos conectar directamente a sus 
respectivos puertos, es decir, a partir de estos cambios hay que usar siempre 
la dirección del ProxyPass:


[root@quercus ~]# cat /etc/httpd/conf.d/kibana.conf 
ProxyPreserveHost On
ProxyRequests On
RewriteRule ^/kibana$ kibana/ [R]
ProxyPass /kibana http://quercus.prib.upf.edu:5601
ProxyPassReverse /kibana http://quercus.prib.upf.edu:5601

<Location "/kibana">
  AuthType Basic
  AuthName "Authentication Required"
  AuthUserFile "/etc/httpd/conf/.htpasswd"
  Require valid-user

  Order allow,deny
  Allow from all
</Location>

[root@quercus ~]#


[root@quercus ~]# cat /etc/httpd/conf.d/grafana.conf 
ProxyPreserveHost On
ProxyRequests On
RewriteRule ^/grafana$ grafana/ [R]
ProxyPass /grafana http://localhost:3000
ProxyPassReverse /grafana http://localhost:3000

[root@quercus ~]#


[root@quercus ~]# cat /etc/httpd/conf.d/elasticsearch-HQ.conf 
ProxyPreserveHost On
ProxyRequests On
RewriteRule ^/elastichq$ elastichq/ [R]

ProxyPass /elastichq http://localhost:5000/
ProxyPassReverse /elastichq http://localhost:5000/

<Location "/elastichq">
  AuthType Basic
  AuthName "Authentication Required"
  AuthUserFile "/etc/httpd/conf/.htpasswd"
  Require valid-user

  Order allow,deny
  Allow from all
</Location>

[root@quercus ~]# 

y los cambios hecho en los ficheros de configuración de kibana y grafana para que 
esto funcione son:

[root@quercus ~]# grep root_url /etc/grafana/grafana.ini 
root_url = http://quercus.prib.upf.edu/grafana
[root@quercus ~]# 

[root@quercus ~]# grep server.basePath /etc/kibana/kibana.yml 
server.basePath: "/kibana"
# `server.basePath` or require that they are rewritten by your reverse proxy.
[root@quercus ~]# 

6.- Beats: FileBeat y MetricBeat
================================

Los beats son una alternativa más sencilla para no tener que usar el logstash 
para que recopile la info de métricas o logs y los envie al elasticsearch. Hay
dos:

- metricbeat: el que usamos en el sit. Es para obtener métricas sobre el estado de
la memoria, cpu, disco, etc y que todo esto lo inserte en el elasticsearch.

- filebeat: este es para leer ficheros de logs y enviarlos elasticsearch

Ambos beats funcionan a base de modulos. Hay que que ponerlos en modo activo para
que monitoricex las métricas o los logs de los servicios que nos interesen: apache, 
mysql, sistema, etc.

En este mismo directorio hay un documento sobre como se instalan estos servicios.


